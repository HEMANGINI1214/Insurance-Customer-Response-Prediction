# -*- coding: utf-8 -*-
"""CAPSTONE PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIjtSKx3PKyY9t1z9VbaF95cwhAJ0xDY

**Insurance Customer Response Prediction**
"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,precision_score,mean_squared_error
from sklearn.metrics import recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Importing Data
train_data = pd.read_csv('/content/train.csv')

train_data.head()

"""
# **EXPLORATRY DATA ANALYSIS**"""

# Shows the Dimension of Dataframe|
train_data.shape

train_data.describe()

# Shows the Dataframe's structure
train_data.info()

# Finds if there are any null values
train_data.isnull().sum()

# Finds the duplicated values in dataframe
train_data.duplicated()

train_data["Gender"].value_counts()

train_data["Previously_Insured"].value_counts()

train_data["Response"].value_counts()

train_data["Vehicle_Damage"].value_counts()

"""**VIZUALISATION DATA**"""

# Plotting the Overall Gender Count

sns.countplot(x="Gender", data=train_data, palette=["Blue", "Green"])
plt.title("GENDER COUNT")
plt.show()

sns.histplot(train_data["Age"],bins= 30 ,kde=True)
plt.title("AGE DISTRIBUTION")
plt.show()

sns.countplot(x="Response", data=train_data, palette=["Red", "Green"])
plt.title("RESPONSE COUNT")
plt.show()

sns.barplot(x='Vehicle_Age', y='Annual_Premium', data=train_data, palette=["Blue", "Red", "Green"])
plt.title("Average Premium by Vehicle Age")
plt.show()

train_data['Vehicle_Age'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title("Vehicle Age Distribution")
plt.show()

"""**DATA PREPROCESSING**"""

df=pd.get_dummies(train_data,columns=['Gender','Vehicle_Age'])
df.head()

df['Vehicle_Damage']=df.Vehicle_Damage.map({'Yes':1,'No':0})
df.head()

df.corr()

plt.figure(figsize=(12,12))
correlation_matrix=df.corr()
sns.heatmap(correlation_matrix,annot=True, cmap='cool',linewidths=5)
plt.title('correlation_matrix')
plt.show()
plt.savefig('correlation_matrix.png')

"""**DECISION TREE**"""

x=df[['id', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured','Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage', 'Vehicle_Age_1-2 Year',
                        'Vehicle_Age_< 1 Year','Vehicle_Age_> 2 Years', 'Gender_Female', 'Gender_Male']]
y=df['Response']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
model=DecisionTreeClassifier()
model.fit(x_train,y_train)

# Make a prediction on test set
pred = model.predict(x_test)
pred

#Evaluate the Model's Performance
Accuracy=accuracy_score(y_test,pred)
Precision=precision_score(y_test,pred)
Recall=recall_score(y_test,pred)
Confusion=confusion_matrix(y_test,pred)
f1=f1_score(y_test,pred)
rmse=np.sqrt(mean_squared_error(y_test,pred))

print('Accuracy Score: {:.2f}%'.format(Accuracy *100))
print('Presicion Score: {:.2f}%'.format(Precision *100))
print('Recall Score: {:.2f}%'.format(Recall*100))
print('F1 Score: {:.2f}%'.format(f1*100))
print('Root Mean Square Error: {:.2f}'.format(rmse))
print('Classification Report: \n',classification_report(y_test,pred))

"""**Random Forest**"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
model=RandomForestClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

Accuracy=accuracy_score(y_test,y_pred)
Precision=precision_score(y_test,y_pred)
Recall=recall_score(y_test,y_pred=pred)
Confusion=confusion_matrix(y_test,y_pred)
f1=f1_score(y_test,y_pred)
rmse=np.sqrt(mean_squared_error(y_test,y_pred))

print('Accuracy Score: {:.2f}%'.format(Accuracy *100))
print('Presicion Score: {:.2f}%'.format(Precision *100))
print('Recall Score: {:.2f}%'.format(Recall*100))
print('F1 Score: {:.2f}%'.format(f1*100))
print('Root Mean Square Error: {:.2f}'.format(rmse))
print('Classification Report: \n',classification_report(y_test,y_pred))

"""**K-Nearest Neighbors(KNN)**"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
model=KNeighborsClassifier()
model.fit(x_train,y_train)

ypred=model.predict(x_test)
ypred

Accuracy=accuracy_score(y_test,ypred)
Precision=precision_score(y_test,ypred)
Recall=recall_score(y_test,pred)
Confusion=confusion_matrix(y_test,ypred)
f1=f1_score(y_test,ypred)
rmse=np.sqrt(mean_squared_error(y_test,ypred))

print('Accuracy Score: {:.2f}%'.format(Accuracy *100))
print('Presicion Score: {:.2f}%'.format(Precision *100))
print('Recall Score: {:.2f}%'.format(Recall*100))
print('F1 Score: {:.2f}%'.format(f1*100))
print('Root Mean Square Error: {:.2f}'.format(rmse))
print('Classification Report: \n',classification_report(y_test,ypred))

"""**Logistic Regression**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
model=LogisticRegression()
model.fit(x_train, y_train)

pred_y = model.predict(x_test)
pred_y

Accuracy=accuracy_score(y_test,pred_y)
Precision=precision_score(y_test,pred_y)
Recall=recall_score(y_test, pred_y)
Confusion=confusion_matrix(y_test,pred_y)
f1=f1_score(y_test,pred_y)
rmse=np.sqrt(mean_squared_error(y_test,pred_y))

print('Accuracy Score: {:.2f}%'.format(Accuracy *100))
print('Presicion Score: {:.2f}%'.format(Precision *100))
print('Recall Score: {:.2f}%'.format(Recall*100))
print('F1 Score: {:.2f}%'.format(f1*100))
print('Root Mean Square Error: {:.2f}'.format(rmse))
print('Classification Report: \n',classification_report(y_test,pred_y))

"""**XGBoost**"""

# Split the data into features (X) and target (y)
X = df.drop('Response', axis=1)
y = df['Response']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Clean column names for XGBoost compatibility
x_train.columns = x_train.columns.str.replace(r'[<>]', '', regex=True)
x_test.columns = x_test.columns.str.replace(r'[<>]', '', regex=True)

# Create an XGBoost model
model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, n_jobs=-1)

# Train the model
model.fit(x_train, y_train)

# Make predictions
y_pred = model.predict(x_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
Precision=precision_score(y_test,y_pred)
Recall=recall_score(y_test, y_pred)
f1=f1_score(y_test,y_pred)

print(f'Accuracy: {accuracy*100:.2f}%')
print(f'Precision: {Precision*100:.2f}%')
print(f'Recall: {Recall*100:.2f}%')
print(f'F1 Score: {f1*100:.2f}%')

"""**Model Comparison Plot**"""

# Data for the graph
models = ['Decision Tree', 'Random Forest', 'KNN Classifier', 'Logistic Regression', 'XGBoost']
accuracy = [82.05, 87.06, 86.40, 87.51, 87.51]
precision = [29.08, 40.39, 13.97, 0, 61.54]
recall = [30, 30, 30, 0, 0]
f1_score = [30, 13, 30, 0, 0]

# Set the width of the bars
bar_width = 0.2

# Set the x-axis positions of the bars
x = np.arange(len(models))

# Create the figure and axis
fig, ax = plt.subplots()

# Plot the bars
ax.bar(x - 1.5 * bar_width, accuracy, bar_width, label='Accuracy')
ax.bar(x - 0.5 * bar_width, precision, bar_width, label='Precision')
ax.bar(x + 0.5 * bar_width, recall, bar_width, label='Recall')
ax.bar(x + 1.5 * bar_width, f1_score, bar_width, label='F1-Score')

# Set the x-axis ticks and labels
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=45, ha='right')

# Set the y-axis label
ax.set_ylabel('Percentage (%)')

# Set the title
ax.set_title('Model Comparison')

# Add a legend
ax.legend()

# Show the plot
plt.tight_layout()
plt.show()
plt.savefig('Model Comparison.png')

from sklearn.metrics import roc_curve, auc, roc_auc_score


# Define the models
models = {
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN Classifier': KNeighborsClassifier(),
    'Logistic Regression': LogisticRegression(),
    'XGBoost': XGBClassifier()
}

# Plot the AUC-ROC curve
plt.figure(figsize=(10, 8))

for name, model in models.items():
    model.fit(x_train, y_train)
    y_pred_proba = model.predict_proba(x_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AUC-ROC Curve for Insurance Customer Response Prediction')
plt.legend()
plt.show()
plt.savefig('AUC-ROC Curve.png')

